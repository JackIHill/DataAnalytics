{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de07dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064b3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82478f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_posts = pd.read_csv(cwd + '//AAPL Reddit Data//five-years-of-aapl-on-reddit-posts.csv')\n",
    "raw_comments = pd.read_csv(cwd + '//AAPL Reddit Data//five-years-of-aapl-on-reddit-comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bdced5",
   "metadata": {},
   "source": [
    "##### Check no comments were erroneously added to the 'post' csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21f8a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw_posts dataframe contains the types: ['post']\n",
      "The raw_comments dataframe contains the types: ['comment']\n"
     ]
    }
   ],
   "source": [
    "print(f\"The raw_posts dataframe contains the types: {raw_posts['type'].unique()}\")\n",
    "print(f\"The raw_comments dataframe contains the types: {raw_comments['type'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2bcc0",
   "metadata": {},
   "source": [
    "#### All good.\n",
    "##### A quick check of posts in nsfw subreddits tells us that the popularity of such posts is low (not exceeding a score of 11). For this reason, and that nsfw subreddits are unlikely to contain serious discussion of the AAPL ticker, they are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224b9979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit.name    worldpolitics\n",
       "score                        11\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_posts[raw_posts['subreddit.nsfw'] == True][['subreddit.name', 'score']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55ef18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_posts = raw_posts[raw_posts['subreddit.nsfw'] == False].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c4986",
   "metadata": {},
   "source": [
    "##### Drop unneeded columns from each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c99589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_posts_comms = [raw_posts, raw_comments]\n",
    "columns=['type', 'subreddit.id', 'subreddit.nsfw', 'permalink', 'domain', 'url']\n",
    "\n",
    "def drop_columns(dfs, columns):\n",
    "    for df in dfs:\n",
    "        for col in columns:\n",
    "            if col in list(df.columns):\n",
    "                df.drop(columns=col, inplace=True)\n",
    "            else:\n",
    "                print('No columns could be dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9c541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns could be dropped\n",
      "No columns could be dropped\n"
     ]
    }
   ],
   "source": [
    "drop_columns(raw_posts_comms, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f596433e",
   "metadata": {},
   "source": [
    "##### Convert unix time values to datetime objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829d14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848e4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(dfs):\n",
    "    for df in dfs:\n",
    "        df['created_utc'] = df['created_utc'].apply(\n",
    "            lambda x: datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        df.rename(columns={'created_utc': 'time_created'}, inplace=True)\n",
    "\n",
    "        df['time_created'] = pd.to_datetime(df['time_created'])\n",
    "        \n",
    "def add_time_cols(dfs):\n",
    "    for df in dfs:\n",
    "        df['year_created'] = df['time_created'].dt.year\n",
    "        df['month_created'] = df['time_created'].dt.month\n",
    "        df['day_created'] = df['time_created'].dt.day\n",
    "        \n",
    "        df.rename(\n",
    "            columns={'time_created': 'date_created',\n",
    "                     'subreddit.name': 'subreddit',\n",
    "                    }, inplace=True)\n",
    "        \n",
    "        df['hour_created'] = df['date_created'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_datetime(raw_posts_comms)\n",
    "add_time_cols(raw_posts_comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f782747",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts = raw_posts.copy()\n",
    "clean_comms = raw_comments.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e5f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580bec5",
   "metadata": {},
   "source": [
    "##### Add column for the count of posts for a given subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb0092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_posts['sub_posts'] = clean_posts.groupby('subreddit')['id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_subs = clean_posts.groupby('subreddit')['sub_posts'].max().reset_index()\n",
    "pop_subs = pop_subs.sort_values('sub_posts', ascending=False).set_index('subreddit')\n",
    "pop_subs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f7974",
   "metadata": {},
   "source": [
    "## <ins>Start of Post Analysis</ins>\n",
    "### How do upvotes change over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(df, x_col, y_col, line=False, **kwargs):\n",
    "    if line is True:\n",
    "        for i, df in enumerate(df):\n",
    "            plt.plot(df[x_col], df[y_col], label=labels[i], color=colors[i])\n",
    "    \n",
    "    elif line == False:\n",
    "        for d in df:\n",
    "            plt.bar(df[x_col], df[y_col], **kwargs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd779946",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_post_scores = (\n",
    "    clean_posts\n",
    "        .groupby(['year_created', 'month_created'])['score']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "# month_mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cac2b9",
   "metadata": {},
   "source": [
    "##### Quick check for high 2017 January mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841bcbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_posts[(clean_posts['month_created'] == 1) & (clean_posts['year_created'] == 2017)]\n",
    "# clean_posts.sort_values(['score'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498715d",
   "metadata": {},
   "source": [
    "#### Anomaly is high scoring post(s) in WSB reddit. I chose to filter out WSB posts due to 1) a much higher volume of posts 2) the top post(s) was/were 'baiting' upvotes (if you upvote, I'll...), and 3) most importantly to reduce bias, I will later perform analysis on posts from solely WSB, in combination with the non-WSB data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d744ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSB_posts = clean_posts[clean_posts['subreddit'] == 'wallstreetbets']\n",
    "clean_posts = clean_posts[clean_posts['subreddit'] != 'wallstreetbets']\n",
    "WSB_comments = clean_comms[clean_comms['subreddit'] == 'wallstreetbets']\n",
    "clean_comms = clean_comms[clean_comms['subreddit'] != 'wallstreetbets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7bb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6242b79",
   "metadata": {},
   "source": [
    "##### Get dataframes with average and highest score for each month for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_post_scores, monthly_comm_scores = clean_posts.copy(), clean_comms.copy()\n",
    "\n",
    "for df in [monthly_post_scores, monthly_comm_scores]:\n",
    "    df['year_month'] = df['date_created'].dt.strftime('%Y-%m')\n",
    "    df['monthly_mean'] = df.groupby('year_month')['score'].transform('mean')\n",
    "    df['monthly_max'] = df.groupby('year_month')['score'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354fe57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax2=ax.twinx()\n",
    "\n",
    "ax.plot(monthly_post_scores['year_month'], monthly_post_scores['monthly_mean'], color='k')\n",
    "ax2.plot(monthly_post_scores['year_month'], monthly_post_scores['monthly_max'], color='tab:orange', linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Upvotes')\n",
    "ax.set_title('Average Upvotes of $AAPL Related Posts on Reddit over Time')\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    \n",
    "ax2.set_ylabel('Max Upvotes', color='tab:orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34794d",
   "metadata": {},
   "source": [
    "I'll later analyse this graph in combination with stock data, so no comments for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9805fab",
   "metadata": {},
   "source": [
    "### Which subreddits have the highest average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9489481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_avg = (\n",
    "    clean_posts\n",
    "        .groupby('subreddit')['score']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .sort_values('score', ascending=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bdc8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_plot(top_avg.head(20), 'subreddit', 'score', color='dimgrey', edgecolor='k')\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Subreddit Name')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Top 20 Subreddits with Highest Average Scores\\n for $AAPL Related Posts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6fb5e",
   "metadata": {},
   "source": [
    "### Which subreddits have the highest scoring posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e162e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_max = (\n",
    "    clean_posts\n",
    "        .groupby('subreddit')['score']\n",
    "        .max()\n",
    "        .reset_index()\n",
    "        .sort_values('score', ascending=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e51e77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_plot(top_max.head(20), 'subreddit', 'score', color='dimgrey', edgecolor='k')\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Subreddit Name')\n",
    "plt.ylabel('Max Score')\n",
    "plt.title('Top 20 Subreddits with Highest Scores\\n for $AAPL Related Posts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc02d1",
   "metadata": {},
   "source": [
    "##### For some extra insight, I plot the prior two graphs again, but this time overlaid with each sub's max, or average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c09f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_max_scores = pd.merge(left=top_avg, right=top_max, how='inner', left_on='subreddit', right_on='subreddit')\n",
    "avg_max_scores.rename(columns={'score_x': 'average', 'score_y': 'max'}, inplace=True)\n",
    "\n",
    "top_20_avg_sort = avg_max_scores.sort_values('average', ascending=False).head(20)\n",
    "top_20_max_sort = avg_max_scores.sort_values('max', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots()\n",
    "ax1 = ax.twinx()\n",
    "\n",
    "ax.bar(top_20_avg_sort['subreddit'], top_20_avg_sort['average'], color='dimgrey', edgecolor='k')\n",
    "ax1.bar(top_20_avg_sort['subreddit'], top_20_avg_sort['max'], color='r', edgecolor='k', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Subreddit')\n",
    "ax.set_ylabel('Average Score')\n",
    "ax1.set_ylabel('Highest Score', color='r')\n",
    "\n",
    "ax.set_title('Average and Highest Upvotes by 20 Highest Average Scoring\\n$AAPL Related Posts on Reddit')\n",
    "\n",
    "ax.tick_params('x', labelrotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409a9e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1, ax = plt.subplots()\n",
    "ax1 = ax.twinx()\n",
    "\n",
    "ax.bar(top_20_max_sort['subreddit'], top_20_max_sort['max'], color='dimgrey', edgecolor='k')\n",
    "ax1.bar(top_20_max_sort['subreddit'], top_20_max_sort['average'], color='r', edgecolor='k', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Subreddit')\n",
    "ax.set_ylabel('Highest Score')\n",
    "ax1.set_ylabel('Average Score', color='r')\n",
    "\n",
    "ax.set_title('Average and Highest Upvotes by 20 Highest Scoring\\n$AAPL Related Posts on Reddit')\n",
    "\n",
    "ax.tick_params('x', labelrotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f883f",
   "metadata": {},
   "source": [
    "No notable correlations here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b487f",
   "metadata": {},
   "source": [
    "### Do certain months receive higher scores?\n",
    "\n",
    "Prior insights (simply looking at the most popular subreddits and their top scores, etc.) didn't warrant filtering the data too much, but we should now consider it given the results of those insights - the top scoring subs curtail after around 10 subs, and so taking a (separate) slice (as explained below) will better represent the majority of answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713ff6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.hist(clean_posts['score'], bins=[_ for _ in np.arange(0, 100, 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e4338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"The first 71% of values are equal to or less than {clean_posts['score'].quantile(0.71)}\")\n",
    "print(f\"The first 72% of values are equal to or less than {clean_posts['score'].quantile(0.72)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fac0b",
   "metadata": {},
   "source": [
    "The vast majority (72%) of values are less than or equal to 1.\n",
    "\n",
    "I therefore create a new series for top posts, just to get an idea of trends, while keeping the clean_posts data as itself. This allows me to more easily compare the top posts to the whole dataset, where the axes is not terribly skewed by the (very) top values. This series contains values between the top 10% and top 1% of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pctl_90 = clean_posts['score'].quantile(0.9)\n",
    "pctl_99 = clean_posts['score'].quantile(0.99)\n",
    "\n",
    "top_posts = clean_posts[(clean_posts['score'] > pctl_90) & (clean_posts['score'] > pctl_90)]\n",
    "month_scores = top_posts.groupby(['month_created', 'year_created'])['score'].mean().reset_index()\n",
    "\n",
    "print(f'90th Percentile: {pctl_90}')\n",
    "print(f'99th Percentile: {pctl_99}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af48397",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(top_posts['score'], bins=[_ for _ in np.arange(0, 190, 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1e45e",
   "metadata": {},
   "source": [
    "Data is still positively skewed, but notably less so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9dfd6",
   "metadata": {},
   "source": [
    "#### From hereon, posts with scores which lie between the 90th and 99th percentile will be referred to as 'top posts' in commentary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4719502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display      \n",
    "\n",
    "colors = {2017:'#3bb410', 2018:'#e9da10', 2019:'#df951e', 2020:'#d2622f', 2021:'#c44141'}\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "\n",
    "plt.xticks(month_scores['month_created'])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Score')\n",
    "plt.yticks(range(0, 1000, 100))\n",
    "plt.title('Average of Scores for Top Scorers, by Month')\n",
    "\n",
    "for year in years:\n",
    "    plt.plot(\n",
    "        month_scores[month_scores['year_created'] == year]['month_created'],\n",
    "        month_scores[month_scores['year_created'] == year]['score'],\n",
    "        color=colors[year],\n",
    "        label=year)\n",
    "          \n",
    "    plt.legend(loc='upper left')\n",
    "        \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af086e56",
   "metadata": {},
   "source": [
    "#### Re-run this cell to animate!\n",
    "\n",
    "Average score seems to spike around the middle of the year, notably so between May and August. Analysing $AAPL stock data for each month will be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdafe27",
   "metadata": {},
   "source": [
    "### What about weekday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee97be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_weekday = clean_posts.copy()\n",
    "top_weekday = top_posts.copy()\n",
    "\n",
    "for df in [all_weekday, top_weekday]:\n",
    "    df['weekday'] = df['date_created'].dt.strftime('%A') \n",
    "\n",
    "cats = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "all_weekday['all_weekday_mean'] = all_weekday.groupby('weekday')['score'].transform('mean')\n",
    "all_weekday['all_weekday_count'] = all_weekday.groupby('weekday')['id'].transform('count')\n",
    "\n",
    "all_weekday['top_weekday_mean'] = top_weekday.groupby('weekday')['score'].transform('mean')\n",
    "all_weekday['top_weekday_count'] = top_weekday.groupby('weekday')['id'].transform('count')\n",
    "\n",
    "weekday_posts = (all_weekday.groupby('weekday').mean().reindex(cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eff8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23371ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "ax.plot(weekday_posts.index, weekday_posts['all_weekday_count'], color='k')\n",
    "ax2.plot(weekday_posts.index, weekday_posts['all_weekday_mean'], color='tab:orange')\n",
    "\n",
    "ax.set_xlabel('Weekday')\n",
    "ax.set_ylabel('Number of Posts')\n",
    "ax2.set_ylabel('Average Score', color='tab:orange')\n",
    "\n",
    "ax.set_title('Number and Average Score of All Posts by Weekday')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59423d",
   "metadata": {},
   "source": [
    "As the week progresses, the number of posts stays consistent until the weekend. This is expected; market is closed on the weekend (thus no concurrent activity to discuss). The average score of posts decreaes across the week, and rises again on Monday. Potential reasons could include excitement for stock market re-open, people not wanting to hold short term positions over the weekend and therefore not interacting with posts, or people deciding/discussing their positions at the start of the week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be001762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "ax.plot(weekday_posts.index, weekday_posts['top_weekday_count'], color='k')\n",
    "ax2.plot(weekday_posts.index, weekday_posts['top_weekday_mean'], color='tab:orange')\n",
    "\n",
    "ax.set_xlabel('Weekday')\n",
    "ax.set_ylabel('Number of Posts')\n",
    "ax2.set_ylabel('Average Score', color='tab:orange')\n",
    "\n",
    "ax.set_title('Number and Average Score of Top Posts by Weekday')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e0e53",
   "metadata": {},
   "source": [
    "The top posts follow a very similar trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718ccf9",
   "metadata": {},
   "source": [
    "### How do the score and count of posts vary by time of day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hour_count = clean_posts.groupby('hour_created')['id'].count().reset_index()\n",
    "all_hour_avg_score = clean_posts.groupby('hour_created')['score'].mean().reset_index()\n",
    "\n",
    "top_hour_avg_score = top_posts.groupby('hour_created')['score'].mean().reset_index()\n",
    "top_hour_avg_count = top_posts.groupby('hour_created')['id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28c4f2",
   "metadata": {},
   "source": [
    "##### Set larger fig size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd652639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb76d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "ax.plot(\n",
    "    all_hour_count['hour_created'],\n",
    "    all_hour_count['id'],\n",
    "    color='k',\n",
    "    label='Posts/Hour'\n",
    ")\n",
    "ax2.plot(\n",
    "    top_hour_avg_score['hour_created'],\n",
    "    top_hour_avg_score['score'],\n",
    "    label='Avg Score/Hour (90th-99th PCTL)',\n",
    "    linestyle='dotted',\n",
    "    color='r'\n",
    ")\n",
    "ax2.plot(\n",
    "    all_hour_avg_score['hour_created'],\n",
    "    all_hour_avg_score['score'],\n",
    "    label='Avg Score/Hour all Posts',\n",
    "    color='tab:orange'\n",
    ")\n",
    "\n",
    "ax.set_xticks(all_hour_count['hour_created'])\n",
    "ax.set_xlabel('Hour (UTC)')\n",
    "ax.set_ylabel('Number of Posts')\n",
    "ax.set_title('Number of $AAPL-related Posts by Hour\\nand their Mean Score')\n",
    "\n",
    "ax2.set_ylabel('Average Score', color='r')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax2.legend(handles + handles2, labels + labels2, loc='upper left')\n",
    "\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b3890",
   "metadata": {},
   "source": [
    "To a degree, it seems that the highest scoring posts lag behind when the number of posts is highest. That is, posting between the hours of 9am and 12pm, before the post-quantity upturn between 11am and 4pm, results in the highest scoring $AAPL related posts. Most notably, the quantity of posts begins to steeply rise at NASDAQ pre-market open and LSE market-open (9am UTC), with number of posts beginning to reach its peak at around NASDAQ market open (2:30pm UTC). AAPL-related posts briefly decrease in quantity at 7pm UTC, perhaps a result of dinner times in the United Kingdom, lunch times in the US, or stable market conditions prior to end-of-day trading. Post-quantity sharply decreases following market-close (9pm UTC) until pre-market open.\n",
    "\n",
    "Given that post-score rises in combination with pre-market open, it could be hypothesised that due-diligence posts are at lease partially responsible; discussion/reasoning around market-plays in this period is valuable towards deciding the positions one will take at market open. In tandem, at market close, post-score average takes a turn updwards for the highest scoring posts (and to a lesser degree, for all posts); people are discussing the market conditions for the day, and the positions they will take on the following day. Let's explore that hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431968d",
   "metadata": {},
   "source": [
    "#### Sets figsize back to normal for all subsequent plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c83d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_DD, top_DD, wsb_DD = clean_posts.copy(), top_posts.copy(), WSB_posts.copy()\n",
    "\n",
    "dataframes = [\n",
    "    df[(df['title'].str.contains('DD|diligence|due diligence')==True) \n",
    "       | (df['selftext'].str.contains('DD|diligence|due diligence')==True)\n",
    "      ] for df in [all_DD, top_DD, wsb_DD]\n",
    "    ]\n",
    "\n",
    "mean_dfs = [df.groupby('hour_created')['score'].mean().reset_index() for df in dataframes]\n",
    "count_dfs = [df.groupby('hour_created')['id'].count().reset_index() for df in dataframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1924080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = {0: 'All DD Posts', 1: 'Top DD Posts', 2: 'WSB DD Posts'}\n",
    "colors = {0: 'tab:red', 1: 'tab:orange', 2: 'tab:blue'}   \n",
    "\n",
    "create_plot([df for df in count_dfs], 'hour_created', 'id', line=True, label=labels, color=colors)\n",
    "        \n",
    "plt.xticks(all_hour_count['hour_created'])\n",
    "plt.xlabel('Hour (UTC)')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.title('Number of Due-Diligence Related Posts by Hour')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49d552",
   "metadata": {},
   "source": [
    "DD for all posts (non-WSB) posts increases dramatically at pre-market open (9am UTC). This is expected, given that people are deciding/discussing their positions prior to the trading period. At NASDAQ market open (2:30 UTC), post-volume begins to decrease, and then rises somewhat after market close until midnight UTC. \n",
    "\n",
    "A higher number of Top DD and WSB DD posts are made later in the day (towards market close); for WSB, this might reflect something as little as a US-centric user base, or as much as more sporadic trading behaviour (i.e. more focus on day-trading). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9dffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_plot([df for df in mean_dfs], 'hour_created', 'score', line=True, labels=labels, colors=colors)\n",
    "\n",
    "plt.xticks(all_hour_count['hour_created'])\n",
    "plt.xlabel('Hour (UTC)')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Average Score of Due-Diligence Related Posts by Hour')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6201ec",
   "metadata": {},
   "source": [
    "While there is a relatively little number of posts with 'DD' or 'diligence' mentioned, such posts do exercise leverage during premarket hours on those where DD is not mentioned (see 'Number of $AAPL-related Posts by Hour and their Mean Score' plot). See, in particular, the higher average scores for top-DD posts at 4am UTC (end of after-market). WSB are upvoted throughout trading hours when post-quantity is high, but receive a spike in upvotes after market close, despite a decline in post-quantity. Therefore, posting DD to WSB at 11pm UTC will result in the highest likelihood of having a highly upvoted post (a drop in post-quantity but a rise in average score).\n",
    "\n",
    "I won't be exploring this further with statistical testing, as the analysis was just for practice/fun, and analysis of reddit data was really meant to be preliminary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9494762",
   "metadata": {},
   "source": [
    "## <ins>Start of Comment Analysis</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4edd22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monthly_comm_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_comm_scores['ymd'] = monthly_comm_scores['date_created'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23213e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = (\n",
    "    monthly_comm_scores[clean_comms['sentiment'].notna()]\n",
    "    .groupby('ymd')\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "sentiment_mean = sentiment.groupby('year_created')['sentiment'].mean().reset_index()\n",
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149798db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "#This cell uses code from: https://stackoverflow.com/questions/23994020/colorplot-that-distinguishes-between-positive-and-negative-values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "  \n",
    "cmap = plt.get_cmap('RdYlGn')\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)\n",
    "\n",
    "# define the bins and normalize and forcing 0 to be part of the colorbar.\n",
    "bounds = np.arange(np.min(-0.4),np.max(0.4),.03)\n",
    "idx=np.searchsorted(bounds,0)\n",
    "bounds=np.insert(bounds,idx,0)\n",
    "norm = BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "ax.scatter(sentiment['ymd'],\n",
    "           sentiment['sentiment'],\n",
    "           c=sentiment['sentiment'], norm=norm, cmap=cmap)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sentiment')\n",
    "ax.set_yticks(np.arange(-0.4, 0.8, 0.2))\n",
    "ax.set_title('Daily Average Sentiment Distribution\\nfor $AAPL-related Comments by Month and Year')\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=1, interval=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780eee6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    sentiment['year_created'],\n",
    "    sentiment['sentiment'],\n",
    "    c=sentiment['sentiment'], norm=norm, cmap=cmap)\n",
    "\n",
    "plt.scatter(\n",
    "    sentiment_mean['year_created'],\n",
    "    sentiment_mean['sentiment'],\n",
    "    color='lightblue', edgecolor='k',\n",
    "    label='Mean')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Daily Average Sentiment Distribution\\nfor $AAPL-related Comments by Year')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sentiment['sentiment'], sentiment['score'], c=sentiment['sentiment'], norm=norm, cmap=cmap)\n",
    "\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Daily Average Sentiment by Score\\nDistribution for $AAPL-related Comments')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b996586",
   "metadata": {},
   "source": [
    "## <ins>Start of Stock Analysis</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717cce2",
   "metadata": {},
   "source": [
    "### Use mplfinance for candlesticks\n",
    "#### How does reddit activity change with stock price?\n",
    "#### How do upvotes change with stock price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e40efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplfinance as mpf\n",
    "\n",
    "raw_stock_data = pd.read_csv(cwd + '//AAPL Stock Data//AAPL-Stock-Data.csv', index_col='Date', parse_dates=True)\n",
    "raw_stock_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de08edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd008100",
   "metadata": {},
   "source": [
    "#### Unfortunately, we only have date values (no time data). Analysis will therefore only be conducted on post/comment values by day. \n",
    "\n",
    "##### We need to get date values beginning from November 1st 2016, the starting date for posts and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814e065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_stock_data = raw_stock_data[raw_stock_data.index >= '2016-11-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stock_data\n",
    "weekly_stock_data = raw_stock_data.resample('W-Mon').mean()\n",
    "monthly_stock_data = raw_stock_data.resample('M').mean()\n",
    "\n",
    "monthly_stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_monthly = sentiment.groupby(['year_created', 'month_created']).mean().reset_index()\n",
    "sentiment_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9638bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "apds = [mpf.make_addplot(sentiment_monthly['sentiment'],\n",
    "                         width=1,\n",
    "                         color='tab:blue',\n",
    "                         ylabel='Average Sentiment'),\n",
    "        mpf.make_addplot(sentiment_monthly['score'],\n",
    "                         width=1,\n",
    "                         color='tab:orange',\n",
    "                         panel=1,\n",
    "                         ylabel='Averge Score', linestyle='--')\n",
    "       ]\n",
    "\n",
    "s = mpf.make_mpf_style(base_mpf_style='yahoo', y_on_right=False)\n",
    "\n",
    "mpf.plot(monthly_stock_data, type='candlestick', style=s, addplot=apds, figscale=1, ylabel='Price (USD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74d173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
